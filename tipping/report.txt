1.
In this query, a simple select from the Customer table was used in order to test different access paths for simple queries. In this case, our predicate checked a range of cust# values from the table. In the case of oneB.sql, the entire range of possible cust# values was used as the value of the predicate. In the case of oneA.sql, the range was restricted as to only have one value (cust#=4) satisfying the predicate. When comparing the plan files for both of these queries, we can see that the estimated cardinalities are vastly different because of the values of these predicates. This causes the evaluator to have an index scan as part of the access path for oneA.sql, since we are only looking for a restricted number of values, while the access path for oneB.sql does not contain this index scan. Thus the access paths for the two variants are different.

2.
Thinking about this problem logically, we noticed that the aggregation operations would require a sort when a group by clause was used since db2 would need to sort the tuples on the grouping attribute, which in the case of our query was price. It would then need to watch for group boundaries in order to compute the aggregate value for each group. In our query twoB.sql, we compute the most expensive book for each genre (by using all genres as the value of the predicate), which requires an explicit sort of the tuples on genre. In our query twoA.sql, the value of our predicate is a single genre. This means that no explicit sort is required since there is only one value of the grouping attribute. This means a simple scan can be performed, checking if the genre is Education, and seeing if the price is higher than the highest we have seen so far. This is similar to a simple scan to find the max of all books.

3.
In this query, the initial idea was to use a field that had an index on it, since this would be required to get the index nested loop join to work for one of the variants. In the case of the query threeA.sql, an inner join on all three tables is performed by using the book# fields in the Book and Purchase tables and the cust# fields in the Customer and Purchase tables. The resulting query plan shows that a hash join is used in order to join the tables. In the query threeB.sql, instead of performing an inner join on Purchase and Customer, the value of the predicate is changed to find a single customer (the customer with cust#=4). This causes the index to be more useful since we know the location of the row with cust#=4. This causes the index nested loop join to be used instead of hash join.

4.
In this query, we knew that the join order was going to be dependant on the size of the tables being joined in the query since the optimizer will choose the one that will result in a better I/O cost in the end. In both fourA.sql and fourB.sql we perform a simple inner join on the three tables Book, Customer and Purchase. In fourA, we want tuples with the customer number between 4 and 10000, which is practically all of the customers. This caused a join of Customer and Purchase, which was then joined with Book. In fourB, we want only tuples with customer number between 4 and 4 (which is the same as saying equal to 4 but used in order to keep the predicate the same). This caused a join of Purchase and Book, which was then joined with Customer. This was the case because the IO cost would have been higher if the plan joined Customer and Purchase together first and then Book since we were only looking at a single cust# in the case of fourB.sql.

5. This query showcases two problems with exceptional user queries which can be handled by a clever optimizer: redundancy and contradiction. Variant A is an example of contradiction. Since a customer's city cannot be both 'Toronto' and "Hamilton', the optimizer knows that the cardinality should be 0. Rather than perform any table operations, it immediately returns nothing. Variant B is an example of redundancy: a naive optimizer implementation might incorrectly estimate the cardinality of the query using the product of independent probabilities, but db2 correctly eliminates the redundancy. The result is a simple table scan of the Customer relation, while a smaller cardinality estimate might have resulted in a more expensive technique.


Alternative 4.
This query showcases how a shift in cardinality in just a single predicate can drastically affect not only the type of joins, but also their order. The query asks the average sale of books when grouped by city and genre. In variant A, which includes all purchases made from the beginning of 2003, we have three nested hash joins. Since the intersection of Book and Customer has the smallest cardinality and is small enough to fit in memory, it is processed first. This enables a hash join on Purchase with subsequent aggregation. When we adjust the time to the beginning of 2010, the cardinality of matching purchases drops dramatically. This makes nested loops the most efficient option, so Purchase is joined in an index nested loop with Customer, and this acts as the inner relation for a further index nested loop join on books.
